{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5028be-51fb-4d09-b773-cb2756b20234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f1cb08c-d360-4117-826e-01b0e9a68143",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_dataset_from_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/AG/Documents/1/Fire_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load datasetin\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mimage_dataset_from_directory\u001b[49m(\n\u001b[0;32m      6\u001b[0m     dataset_path,\n\u001b[0;32m      7\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,  \u001b[38;5;66;03m# 80% per trajnim, 20% per vertetim\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     10\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m),  \u001b[38;5;66;03m# Resize te gjitha imazhet ne 128x128\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m          \u001b[38;5;66;03m# Load 32 imazhe\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(\n\u001b[0;32m     15\u001b[0m     dataset_path,\n\u001b[0;32m     16\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     21\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_dataset_from_directory' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset path\n",
    "dataset_path = \"C:/Users/AG/Documents/1/Fire_dataset\"\n",
    "\n",
    "# Load datasetin\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,  # 80% per trajnim, 20% per vertetim\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(128, 128),  # Resize te gjitha imazhet ne 128x128\n",
    "    batch_size=32          # Load 32 imazhe\n",
    ")\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a7cbcb-683b-4d49-aa6f-40d879049f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.2),  #Rastesisht fik 20% te neuroneve per te reduktuar overffiting\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Flatten(),  # Rrafshon hartat e vecorive ne nje vektor 1D\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Output 1 neuron per klasifikimin binar\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd1c0a6-8863-4e3a-8abb-106de3f68be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Adam optimizues me norme mesimi 0.001\n",
    "    loss='binary_crossentropy',  # Humbja e klasifikimit binar\n",
    "    metrics=['accuracy']         # Masim duke perdorur saktesine\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b973953-a77c-44f0-8ee5-f52d88c03d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.6000 - loss: 24.8768 - val_accuracy: 0.0000e+00 - val_loss: 443.3388\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.6000 - loss: 385.1339 - val_accuracy: 0.0000e+00 - val_loss: 199.5767\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.6000 - loss: 199.4732 - val_accuracy: 1.0000 - val_loss: 1.0364e-29\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8000 - loss: 99.4831 - val_accuracy: 1.0000 - val_loss: 3.7355e-35\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.4000 - loss: 175.9734 - val_accuracy: 1.0000 - val_loss: 0.0137\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.4000 - loss: 64.1050 - val_accuracy: 0.0000e+00 - val_loss: 38.1743\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.6000 - loss: 41.6092 - val_accuracy: 0.0000e+00 - val_loss: 41.5845\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.6000 - loss: 54.6592 - val_accuracy: 0.0000e+00 - val_loss: 30.0384\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.6000 - loss: 34.2897 - val_accuracy: 0.0000e+00 - val_loss: 15.3692\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8000 - loss: 1.3890 - val_accuracy: 0.0000e+00 - val_loss: 2.3888\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=10  # Trajnim per 10 epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "653b7618-9acf-43a6-813c-4e116c4af629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0000e+00 - loss: 2.3888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Vleresimi ne bazen e te dhenave te vertetimit\n",
    "val_loss, val_accuracy = model.evaluate(validation_dataset)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Ruajme modelin\n",
    "model.save(\"fire_detection_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
